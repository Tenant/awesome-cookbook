# 机器学习

[课件云盘](https://pan.baidu.com/s/1X3fvUFxFNDv9f9nqRmjJOQ), 密码：==2t98==

[UCI-Dataset](https://archive.ics.uci.edu/ml/datasets.html)

[习题解](https://blog.csdn.net/liufei00001/article/details/80973809)

[算法实现](https://github.com/WenDesi/lihang_book_algorithm)

## 1. 机器学习概述

## 2. 感知机

感知机1957年由Rosenblatt提出，是神经网络与支持向量机的基础。

感知机的学习策略是最小化损失函数：$\min_{w,b}L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i +b)$. 损失函数对应于误分类点到分离超平面的总距离。

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-22_182332.png)

感知机学习算法由于采用不同的初值或选取不同的误分类点，解可以不同。当训练数据集线性可分时，感知机学习算法存在无穷多个解。

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-22_183413.png)

当训练数据集线性可分时，感知机学习算法是收敛的。感知机算法在训练数据集上的误分类次数$k$满足不等式$k<=(R/\gamma)^2$.

### 习题

1. 证明：感知机不能表示异或。
2. 证明：设集合$S\in R^n$是由$R^n$中的$k$个点所组成的集合，即$S=\{x_1,x_2,...,x_k\}$. 定义$S$的凸壳$conv(S)$为$conv(S)=\{x=\sum_{i=1}^k\lambda_i x_i | \sum_{i=1}^k \lambda_i=1,i=1,2,...,k\}$.

## 3. $k$邻近法

### 3.1 $k$邻近模型

#### 3.1.1 算法

![alt image](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-22_185145.png)

#### 3.1.2 距离度量

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-22_185421.png)

#### 3.1.3 $k$值的选择

$k$值的减小意味着整体模型变得复杂，容易发生过拟合；$k$值的增大则意味着整体模型变得简单。

#### 3.1.4 分类决策规则

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-12-03_222405.png)

### 3.2 $k$近邻法的实现：$kd$树



**小结**

$k$近邻模型对应于基于训练数据集来对特征空间的一个划分。$k$近邻法中，当训练集、距离度量、$k$值及分类决策规则确定后，其结果唯一确定。

## 4. 朴素贝叶斯法

### 4.1 朴素贝叶斯法的学习与分类

### 4.2 朴素贝叶斯法的参数估计

![alt image](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-22_190618.png)

### 4.3 贝叶斯估计

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-22_191215.png)

**习题**

1. 用极大似然估计法推出朴素贝叶斯法中的概率估计公式$(4.8)$及公式$(4.9)$.

   ![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-12-07_105406.png)

2. 用贝叶斯估计法推出朴素贝叶斯法中概率估计公式$(4.10)$及公式(4.11).

   ![]

## 5. 决策树

### 5.1 特征选择

#### 5.1.1 信息增益

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-22_191851.png)

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-22_192045.png)

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-22_192723.png)

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-22_192326.png)

#### 5.1.2 信息增益比

信息增益$g(D,A)$偏向于取值比较多的特征。

特征$A$对训练数据集$D$的分裂信息$Splitinfo_A(D)$, 定义为$Splitinfo_A(D)=\sum_{i=1}^n \frac{D_i}{D} \log \frac{|D_i|}{|D|}$.

特征$A​$对$D​$的信息增益比$g_R(D,A)=\frac{g(D,A)}{Splitinfo_A(D)}​$.

### 5.2 决策树模型

>  决策树学习本质上是从训练数据集$D$中归纳出一组分类规则，使得
>
> - 与训练数据集矛盾较小
> - 具有很好的泛化能力

>  决策树学习用损失函数(正则化的极大似然函数)表示这一目标。

>  学习策略就是以损失函数为目标函数的最小化。

> 从所有可能的决策树中选取最优决策树是$NP$完全问题。

### 5.3决策树的生成与剪枝

> 如何选择最优特征？==信息增益最大==
>
> 如何增强泛化能力或者防止过拟合？==剪枝==
>
> - 对决策模型进行简化
> - 去掉过于细分的叶结点，将较高层的结点作为叶结点

> 决策树的生成对应于模型的==局部选择==；
>
> 决策树的剪枝对应于模型的==全局选择==。

决策树的剪枝通过==极小化==决策树整体的损失函数或代价函数来实现。

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-30_090217.png)

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-30_091042.png)

### 5.4 分类与回归树

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-30_092026.png)

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-30_092741.png)

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-30_092930.png)

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-30_093127.png)

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-30_093527.png)

==算法停止计算的条件：结点中的样本个数小于预定阈值，或者样本集的基尼指数小于给定阈值，或者没有更多特征==

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-30_095252.png)

### 本章小结

### 习题

[Reference](https://blog.csdn.net/familyshizhouna/article/details/72551841): 证明题可能是乱写的



**5.3** 证明CART算法中，当$\alpha$确定时，存在唯一的最小子树$T_\alpha$使损失函数$C_\alpha (T)$最小。

==**5.4** 证明CART剪枝算法中求出的子树序列$\{T_0,...,T_n\}$分别是区间$\alpha \in [\alpha_i,\alpha_{i+1})$的最优子树$T_\alpha$, 这里$i=0,1,...,n$, $0=\alpha_0<\alpha_1<...<\alpha_n<+\infin$.==

## 6. 逻辑斯谛回归与最大熵模型

### 6.1 逻辑斯谛回归模型

#### 6.1.1 二项逻辑斯谛回归模型

$\log \frac{P(Y=1|x)}{1-P(Y=1|x)}=w \cdot x$

在逻辑斯谛回归中，输出$Y=1$的对数几率是输入$x$的线性函数. 或者说，输出$Y=1$的对数几率是由输入$x$的线性函数表示的模型，即逻辑斯谛回归模型。

#### 6.1.2 模型参数估计

**逻辑斯谛回归模型可以通过对对数似然函数进行最大似然估计推出**

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-25_102934.png)

#### 6.1.2 多项逻辑斯谛回归模型

假设离散型随机变量$Y$的取值集合是$\{1,2,...,K\}$, 那么多项逻辑斯谛回归模型是：

$$\begin{align}P(Y=k|x)=\frac{1}{1+\sum_{k=1}^{K-1} \exp (w_k \cdot x)}\end{align}$$

这里, $x\in R^{n+1}$, $w_k \in R^{n+1}$, $k=1,2,...,K$.

## 6.2 最大熵模型

 #### 6.2.1 最大熵原理

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-25_103820.png)

#### 6.2.2 最大熵模型的定义

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-25_104903.png)

#### 6.2.3 最大熵模型的学习

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-25_105556.png)

#### 6.2.4 极大似然估计

最大熵模型学习中的对偶函数极大化等价于最大熵模型的极大似然估计。

### 6.3 模型学习的最优化算法

#### 6.3.1 改进的迭代尺度法

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-25_114920.png)



## 7. 支持向量机

支持向量机是一种二类分类模型。它的基本模式是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机还包括核技巧，这使它成为实质上的非线性分类器。支持向量机的学习策略就是间隔最大化。

### 7.1 线性可分支持向量机与硬间隔最大化

#### 7.1.1 函数间隔和几何间隔

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-22_193908.png)

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-22_194053.png)

#### 7.1.2 间隔最大化

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-22_194553.png)

上式就是在保证==正确分类数据==前提下，==最小化==模型复杂度。

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-22_194855.png)

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-22_195519.png)

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-22_195636.png)

### 7.2 线性支持向量机与软间隔最大化

#### 7.2.1 线性支持向量机

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-22_200342.png)

#### 7.2.2 学习的对偶算法

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-22_200623.png)

#### 7.2.3 支持向量

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-10-22_201653.png)

#### 7.2.4 合页损失函数

对于线性支持向量机学习来说，其模型为分离超平面$w^* \cdot x + b^* = 0$及决策函数$f(x) = \text{sign} (w^* \cdot x + b^*)$，其学习策略为软间隔最大化，学习算法为凸二次规划。

![](https://github.com/Tenant/Image-Repo/raw/master/2018/2018-12-19_124247.png)

![](https://github.com/Tenant/Image-Repo/raw/master/2018/2018-12-19_124530.png)

### 7.3 非线性支持向量机与核函数

#### 7.3.1 核技巧

#### 7.3.2 正定核

通常所说的核函数就是正定核。

![](https://github.com/Tenant/Image-Repo/raw/master/2018/2018-12-10_113049.png)

![](https://github.com/Tenant/Image-Repo/raw/master/2018/2018-12-10_114602.png)

#### 7.3.3 常用核函数

![](https://github.com/Tenant/Image-Repo/raw/master/2018/2018-12-10_115221.png)

#### 7.3.4 非线性支持向量机

![](https://github.com/Tenant/Image-Repo/raw/master/2018/2018-12-10_115643.png)

### 7.4 序列最小最优化算法

### 习题





## 8. EM Algorithm

![](https://github.com/Tenant/Image-Repo/raw/master/2018/2018-12-11_154207.png)

![h](https://github.com/Tenant/Image-Repo/raw/master/2018/2018-12-11_160233.png)

EM算法是通过不断求解下界的极大化逼近求解对数似然函数极大化的算法。

设$P(Y|\theta)$为$Y$的似然函数，则$P(Y|\theta^{(i+1)})>=P(Y|\theta^{[i]})$

$P(Y|\theta)=\frac{P(Y,Z|\theta)}{P(Z|Y,\theta)}$

$$\begin{align}\log P(Y|\theta)=\log P(Y,Z|\theta)-\log P(Z|Y,\theta)\end{align}$$

$$\begin{align}H(\theta,\theta^{(i)})=\sum_z \log P(Z|Y,\theta) P(Z|Y,\theta^{(i)})\end{align}$$

$$\begin{align}\log P(Y|\theta)=Q(\theta,\theta^{(i+1)})-H(\theta,\theta^{(i)})\end{align}$$

$$\begin{align}\log P(Y|\theta^{i+1})-\log P(Y|\theta^{i})=Q(\theta^{(i+1)},\theta^{(i)})-H(\theta^{(i+1)},\theta^{(i)})-Q(\theta^{(i)},\theta^{(i)})+H(\theta^{(i)},\theta^{(i)})=Q(.)-Q(.)-(H(.)-H(.))\end{align}$$



## 10. 隐马尔可夫模型

### 10.1 隐马尔科夫模型的定义

### 10.2 概率计算算法

已知参数$\lambda$, $A$以及$B$，求$P(O|\lambda)$.

#### 10.2.1 直接计算法

朴素的计算方法共有==$N^T$==条路径，计算的时间复杂度为$O(TN^T)$.

#### 10.2.2 前向计算法

$\alpha_t(i)=P(o_1,o_2,...,o_t,i_t=q_i|\lambda)$

$\alpha_1(i)=\pi_ib_i(o_1)$

$\alpha_{t+1}(i)=[\sum_{i=1}^N \alpha_t(j)a_{ji}]b_i(o_{t+1})$

$P(O|\lambda)=\sum_{i=1}^N \alpha_T(i)$

**前向算法**高效的关键是其==局部计算前向概率==，然后利用路径结构将前向概率“递推”到全局，得到$P(O|\lambda)$. 前向概率计算的复杂度为$O(N^2T)$.

#### 10.2.3 后向计算法

$$\begin{align}\beta_t(i)=P(o_{t+1},o_{t+2},...,o_T|i_t=q_t,\lambda)\end{align}$$

$\beta_T(i)=1$

$\beta_t(i)=\sum_{j=1}^N a_{ij}b_j(o_{t+1})\beta_{t+1}(j)$

$P(O|\lambda)=\sum_{i=1}^N \pi_i b_i(o_1)\beta_1(i)$

==后向算法==的计算基于==条件概率==，最后的等式将条件概率转变为==联合分布==。

利用前向概率和后向概率的定义可以将观测序列概率$P(O|\lambda)$统一写成

$$\begin{align}P(O|\lambda)=\sum_{i=1}^N \sum_{j=1}^N \alpha_t(i)a_{ij}\end{align}b_j(o_{t+1}\beta_{t+1}(j))$$, $t=1,2,...,T-1$

#### 10.2.4 一些概率与期望值的计算

| ——                                                           | ——                                                           |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 给定模型$\lambda$和观测$O$, 在时刻$t$处于状态$q_i$的概率     | $\gamma_t(i)=\frac{\alpha_t(i)\beta_(i)}{\sum_{j=1}^N \alpha_t(j)\beta_t(j)}$ |
| 给定模型$\lambda$和观测$O$, 在时刻$t$处于状态$q_i$且在时刻$t+1$处于状态$q_j$的概率 | $$                                                           |
| 在观测$O$下状态$i$出现的期望值                               | $\sum_{t=1}^T \gamma_t(i)$                                   |
| 在观测$O$下由状态$i$转移的期望值                             | $\sum_{t=1}^T \gamma_t(i)$                                   |
| 在观测$O$下由状态$i$转移到状态$j$的期望值                    | $\sum_{t=1}^{T-1} \xi_t(i,j)$                                |

### 10.3 学习算法

#### 10.3.1 监督学习方法

利用==极大似然估计法==来估计隐马尔可夫模型的参数。

#### 10.3.2 Baum-Welch算法

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-11-08_112109.png)

### 10.4 预测算法

#### 10.4.1 近似算法

近似算法的优点是计算简单，其缺点是不能保证预测的状态序列整体是最有可能的状态序列。

#### 10.4.2 维特比算法

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-11-08_113431.png)

## 11. 提升方法

### 11.1 AdaBoost

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-11-15_025040.png)

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-11-15_030050.png)

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-11-15_032426.png)

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-11-15_032513.png)

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-11-15_034920.png)

### 11.2 提升树

![](http://images-repo.oss-cn-beijing.aliyuncs.com/2018-11-20_093913.png)

### 11.3 Bagging

